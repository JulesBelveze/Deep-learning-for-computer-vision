{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1.2\n",
    "### Classification of MNIST digits with a convolutional neural network\n",
    "\n",
    "In this exercise we will classify MNIST digits again, but this time we will use a convolutional neural network (CNN).\n",
    "\n",
    "First we import the modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that this script has a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on GPU. This is important so things run faster.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU. This is important so things run faster.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. You should probably not do this.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the MNIST dataset, which is built into pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.RandomRotation(100),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST('./data', train=True, download=True, transform=transformer)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "testset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print(\"Augmented training data\")\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a network to classify MNIST digits. \n",
    "The network should consist of two parts, a part with convolutions and one with fully connected layers.\n",
    "The convolutional part we will call `convolutional`, and it should contain the follwing:\n",
    "* two convolutional layers with 8 features\n",
    "* a $2\\times2$ max pooling layer\n",
    "* two convolutional layers with 16 features\n",
    "* a fully connected layer\n",
    "\n",
    "The convolutions should be $3\\times 3$, and should not change the size of the output. What does this mean that the stride and padding should be?\n",
    "\n",
    "Do for example check the documentation of the `nn` module https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU())\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "                nn.Linear(14*14*16, 500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 10),\n",
    "                nn.Softmax(dim=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a copy of our network and transfer it to the GPU if it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model.to(device)\n",
    "#Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this network for five epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d97fd4082984c1594d509ee0ab44081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f8836ac0c94e338ee21d7fed6f416d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 93.1%\t test: 94.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dd2957f7d54aa39b76c977d96af1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 94.1%\t test: 94.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599a0cc08f5240b992558d7180cd91a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 94.7%\t test: 95.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90686e00ebf549859c530a9bb1ebc4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 95.1%\t test: 95.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3570dd2f302644dda223394ab4050fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 95.5%\t test: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "    #For each epoch\n",
    "    train_correct = 0\n",
    "    for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #Zero the gradients computed for each weight\n",
    "        optimizer.zero_grad()\n",
    "        #Forward pass your image through the network\n",
    "        output = model(data)\n",
    "        #Compute the loss\n",
    "        loss = F.nll_loss(torch.log(output), target)\n",
    "        #Backward pass through the network\n",
    "        loss.backward()\n",
    "        #Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Compute how many were correctly classified\n",
    "        predicted = output.argmax(1)\n",
    "        train_correct += (target==predicted).sum().cpu().item()\n",
    "    #Comput the test accuracy\n",
    "    test_correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        predicted = output.argmax(1).cpu()\n",
    "        test_correct += (target==predicted).sum().item()\n",
    "    train_acc = train_correct/len(trainset)\n",
    "    test_acc = test_correct/len(testset)\n",
    "    print(\"Accuracy train: {train:.1f}%\\t test: {test:.1f}%\".format(test=100*test_acc, train=100*train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you now have a model that's able to achieve decent performance on MNIST.\n",
    "It should have around 97.5% accuracy on the test set after the first epoch.\n",
    "\n",
    "* Why does it have higher accuracy on the test set than the training set after the first epoch?\n",
    "\n",
    "After the first epoch the neural net is remembering the specifities of the training set, which it has already met in the first epoch.\n",
    "\n",
    "### Data augmentation\n",
    " * Add random rotations to the MNIST digits during training (you have to go back and modify the dataloader)\n",
    " \n",
    "  hint: you can use `transforms.RandomRotation` \n",
    "  \n",
    "  hint: you can combine multiple transforms into one with `transforms.Compose`\n",
    "\n",
    "How does this affect your training and testing loss?\n",
    "\n",
    " * Try plotting some of the augmented images, to visually confirm what your augmentation is doing.\n",
    "\n",
    " * Try adding another type of data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the model\n",
    "What has the model learned? You can access all the weights in the model with `model.parameters()`. Here we just print the shape.\n",
    " - Try showing images of the filters in the first layer. \n",
    " - Can you from the sizes alone identify which layer it is in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([8, 1, 3, 3]),\n",
       " torch.Size([8]),\n",
       " torch.Size([8, 8, 3, 3]),\n",
       " torch.Size([8]),\n",
       " torch.Size([16, 8, 3, 3]),\n",
       " torch.Size([16]),\n",
       " torch.Size([16, 16, 3, 3]),\n",
       " torch.Size([16]),\n",
       " torch.Size([500, 3136]),\n",
       " torch.Size([500]),\n",
       " torch.Size([10, 500]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.shape for w in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ = [w for w in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training data\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print(\"Augmented training data\")\n",
    "# fig = plt.figure()\n",
    "for i in range(6):\n",
    "    print(example_data[i][0].shape)\n",
    "#     plt.subplot(2,3,i+1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "#     plt.title(\"{}\".format(example_targets[i]))\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    " * Try adding dropout to your model.\n",
    " \n",
    "You can add it between the convolutional layers and or in the fully connected part.\n",
    "\n",
    "Remember to call `net.train()` and `net.eval()` to change the model from test to training state, so it knows when you want it to apply dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
